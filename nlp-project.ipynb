{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nfrom ast import literal_eval\nimport itertools\nfrom itertools import chain\nfrom ast import literal_eval \n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import AutoTokenizer, AutoModel\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nos.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n\ndevice = torch.device('cuda')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://www.kaggle.com/code/iamsdt/pytorch-bert-baseline-nbme/notebook\n\nhttps://www.kaggle.com/code/odins0n/nbme-detailed-eda\n\nhttps://www.kaggle.com/code/tomohiroh/nbme-bert-for-beginners","metadata":{}},{"cell_type":"code","source":"BASE_PATH= '../input/nbme-score-clinical-patient-notes'\n\ndef preprocess_txt(text):\n    text = text.replace('FHx', 'FH ')\n    text = text.replace('FHX', 'FH ')\n    text = text.replace('PMHx', 'PMH ')\n    text = text.replace('PMHX', 'PMH ')\n    text = text.replace('SHx', 'SH ')\n    text = text.replace('SHX', 'SH ')\n    text = text.lower()\n    return text\n\ndef load_and_prepare(path):\n    patient_notes = pd.read_csv(path + \"/patient_notes.csv\")\n    features = pd.read_csv(path + \"/features.csv\")\n    train = pd.read_csv(path + \"/train.csv\")\n    df = train.merge(patient_notes, how = \"left\")\n    df = df.merge(features, how = \"left\")\n\n    df['pn_history']=df['pn_history'].apply(preprocess_txt)\n    df['feature_text']=df['feature_text'].apply(preprocess_txt)\n    \n    df[\"annotation_list\"] = [literal_eval(x) for x in df[\"annotation\"]]\n    df[\"location_list\"] = [literal_eval(x) for x in df[\"location\"]]\n    \n    df = df.loc[df[\"annotation\"] != \"[]\"].copy().reset_index(drop = True)\n    \n\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:50:42.319555Z","iopub.execute_input":"2022-07-19T13:50:42.319936Z","iopub.status.idle":"2022-07-19T13:50:42.333701Z","shell.execute_reply.started":"2022-07-19T13:50:42.319906Z","shell.execute_reply":"2022-07-19T13:50:42.332755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exploring the dataset:\nWe can see there are 9 unique cases. What we are using to train the model are:  the case_num (0 ~ 9, each num belongs their group),pn_num - through pn_num, (detailed history of patient),feature_text(symptons of patient). What we want to predict is the annotation and location. \n\nDescription from hosts\n\ntrain.csv - Feature annotations for 1000 of the patient notes, 100 for each of ten cases.\nid - Unique identifier for each patient note / feature pair.\npn_num - The patient note annotated in this row.\nfeature_num - The feature annotated in this row.\ncase_num - The case to which this patient note belongs.\nannotation - The text(s) within a patient note indicating a feature. A feature may be indicated multiple times within a single note.\nlocation - Character spans indicating the location of each annotation within the note. Multiple spans may be needed to represent an annotation, in which case the spans are delimited by a semicolon ;.","metadata":{}},{"cell_type":"code","source":"train_df= load_and_prepare(BASE_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:50:44.728746Z","iopub.execute_input":"2022-07-19T13:50:44.729208Z","iopub.status.idle":"2022-07-19T13:50:45.605024Z","shell.execute_reply.started":"2022-07-19T13:50:44.729173Z","shell.execute_reply":"2022-07-19T13:50:45.603288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exploring the annotations for a particular patient using spacy:","metadata":{}},{"cell_type":"code","source":"import spacy \npatient_df = train_df[train_df[\"case_num\"] == 0]\nlocation  = patient_df[\"location\"]\nannotation = patient_df[\"annotation\"]\nstart_pos = []\nend_pos = []\nfor i in location:\n    for j in i:\n        start_pos.append(j.split()[0])\n        end_pos.append(j.split()[1])\n        \nents = []\nfor i in range(len(start_pos)):\n    ents.append({\n        'start': int(start_pos[i]), \n        'end' : int(end_pos[i]),\n        \"label\" : \"Annotation\"\n    })\ndoc = {\n    'text' : train_df[train_df[\"case_num\"] == 0][\"pn_history\"].iloc[0],\n    \"ents\" : ents\n}\ncolors = {\"Annotation\" :\"linear-gradient(90deg, #aa9cfc, #fc9ce7)\" } \noptions = {\"colors\": colors}\nspacy.displacy.render(doc, style=\"ent\", options = options , manual=True, jupyter=True);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blank_annotations = df[\"annotation\"] == \"[]\"\nblank_locations = df[\"location\"] == \"[]\"\nboth_blank = (df[\"annotation\"] == df[\"location\"]) & blank_annotations\nprint(sum(blank_annotations), sum(blank_locations), sum(both_blank))","metadata":{"execution":{"iopub.status.busy":"2022-07-19T12:54:29.287525Z","iopub.execute_input":"2022-07-19T12:54:29.287917Z","iopub.status.idle":"2022-07-19T12:54:30.256448Z","shell.execute_reply.started":"2022-07-19T12:54:29.287882Z","shell.execute_reply":"2022-07-19T12:54:30.255125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a lot of missing annotations, meaning this is a semi-supervised/ unsupervised problem. One way we could tackle this, and use this information is through pseudo labelling. For now, I'll remove these.","metadata":{}},{"cell_type":"code","source":"def list_to_int(loc_list):\n    new=[]\n    for str_ in loc_list:\n        strs= str_.split(';')\n        for loc in strs:\n            start,end = loc.split():\n            new.append((int(start),int(end)))\n            \n    return new \n    \n    #Returns tuple with the start and end positions for the annotations","metadata":{"execution":{"iopub.status.busy":"2022-07-19T15:57:36.264868Z","iopub.execute_input":"2022-07-19T15:57:36.265483Z","iopub.status.idle":"2022-07-19T15:57:36.291420Z","shell.execute_reply.started":"2022-07-19T15:57:36.265370Z","shell.execute_reply":"2022-07-19T15:57:36.290575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"QA model, from hugging face- 3 different tokens (special tokens, question tokens, and context tokens). The [CLS] (a special token) comes at the start of the the question. The [sep] (a special token) comes at the start of the context tokens. Each token is then passed through a transfromer encoder and produces a vecotr called a hidden state. Feed them through a linear layer, and then we are left with probabilities that these tokens are the start/end toekn associated with the answer. Training--> learns to classify which pair of tokens is the start/end. \nEach token gets 2 labels, a start and end label. If the first label for a token is 1, that indicates that this is the start. If the secondf label is a 1, that idicates this toke is an end token. \nOffset is the character positioning. ","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"../input/huggingface-bert/bert-base-uncased\" \ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The tokenizer returns a dictionary with all the arguments necessary for its corresponding model to work properly. The token indices are under the key “input_ids”:\nNote that the tokenizer automatically adds “special tokens” (if the associated model relies on them) which are special IDs the model sometimes uses. (These are the CLS, SEP tokens mentioned earlier). \nThe attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them. For the BertTokenizer, 1 indicates a value that should be attended to, while 0 indicates a padded value. This attention mask is in the dictionary returned by the tokenizer under the key “attention_mask”:","metadata":{}},{"cell_type":"code","source":"def tokenize_labels(tokenizer, example):  #Returns a dictionary with: input_ids, attention masks, offset_mapping and token_type ids. We then add sequence_ids, location int and labels. \n    tokenized_inputs = tokenizer(\n        question= example[\"feature_text\"], #features are the question\n        context= example[\"pn_history\"], #patient history is the context, annotations are the answers\n        truncation = \"only_second\",\n        max_length = 416, # max length is 406, dont need to return overflowing tokens as 406<512\n        padding = \"max_length\",\n        return_offsets_mapping = True #Allow us to compute the start and end positions of annotation\n    )\n    labels = [0.0] * len(tokenized_inputs[\"input_ids\"]) \n    \n    tokenized_inputs[\"location_int\"] = list_to_int(example[\"location_list\"])\n    tokenized_inputs[\"sequence_ids\"] = tokenized_inputs.sequence_ids()   #add 2 sequences, question and context. If seq_id is None, that token is a special token (or padding). It can be either 1 (for context) or 0 (for q)\n\n    for idx, (seq_id, offsets) in enumerate(zip(tokenized_inputs[\"sequence_ids\"], tokenized_inputs[\"offset_mapping\"])):\n        if seq_id is None or seq_id == 0:\n            labels[idx] = -1          \n            continue\n        \n        token_start, token_end = offsets\n        for feature_start, feature_end in tokenized_inputs[\"location_int\"]:\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                break\n    tokenized_inputs[\"labels\"] = labels\n    \n    return tokenized_inputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def location_preds(preds, offset_mapping, sequence_ids, test=False):\n    all_preds=[]\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids): #for batch in batches\n        pred= 1/(1+np.exp(-pred))  #sigmoid function->> outputs a probability (between 0 and 1), used for binary classification\n        start_idx= None\n        end_idx= None\n        current_preds=[]\n        for pred, offset, seq_id in zip(pred, offsets, seq_ids): #for each token \n            if seq_id is None or seq_id == 0:   #i.e it is a special token\n                continue\n                \n            if pred > 0.5:\n                if start_idx is None:  #if none, then this offset will be the first \n                    start_idx = offset[0]\n                end_idx = offset[1] #unless another token with pred>0.5 comes after, final offset\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds)) #if more than 1 annotation\n        else:\n            all_predictions.append(current_preds)\n            \n    return all_predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_char_cv(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n\n        num_chars = max(list(chain(*offsets))) #MAX-> the maximum number= the number of charcaters, as max offset is the end.  \n        char_labels = np.zeros(num_chars) #remember location indices only wrt to characters, not tokens\n\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n\n        char_preds = np.zeros(num_chars)\n\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n\n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n\n    results = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", labels=np.unique(all_preds))\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n\n    return {\n        \"Accuracy\": accuracy,\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compute precision, recall, F-measure and support for each class.\n\nThe precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label a negative sample as positive.\n\nThe recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n\nThe F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n\nThe F-beta score weights recall more than precision by a factor of beta. beta == 1.0 means recall and precision are equally important.\n\nThe support is the number of occurrences of each class in y_true.","metadata":{}},{"cell_type":"markdown","source":"Loading the data in","metadata":{}},{"cell_type":"code","source":"class NBMEDataset(Dataset):\n    def __init__(self,data, tokenizer):\n        self.data=data\n        self.tokenizer= tokenizer\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = tokenize_labels(self.tokenizer, example)\n\n        input_ids = np.array(tokenized[\"input_ids\"]) # for input BERT\n        attention_mask = np.array(tokenized[\"attention_mask\"]) # for input BERT\n        labels = np.array(tokenized[\"labels\"]) # to calculate loss and cv score\n\n        offset_mapping = np.array(tokenized[\"offset_mapping\"]) # to calculate cv score\n        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\") # to calculate cv score\n        \n        return input_ids, attention_mask, labels, offset_mapping, sequence_ids","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NBMEModel(Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(MODEL_NAME)\n        self.dropout = torch.nn.Dropout(p = 0.2)\n        \n        self.layer = torch.nn.Linear(768, 1) # BERT has last_hidden_state(size: sequence_length, 768)\n    \n    def forward(self, input_ids, attention_mask):\n        last_hidden_state = self.bert(input_ids = input_ids, attention_mask = attention_mask,token_type_ids=token_type_ids)[0] \n        logits = self.layer(self.dropout(last_hidden_state)).squeeze(-1)\n        return logits","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test = train_test_split(train_df, test_size=0.2,\n                                   random_state=42)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainig_data= NBMEDataset(X_train, tokenizer)\ntrain_dataloader = DataLoader(training_data, batch_size=8, shuffle=True)\nvalid_data= NBMEDataset(X_test, tokenizer, test=True )\nmodel = NBMEModel().to(DEVICE)\ncriterion = torch.nn.BCEWithLogitsLoss(reduction='none')\noptimizer = torch.optim.AdamW(model.parameters(), lr = 1e-5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.","metadata":{}},{"cell_type":"markdown","source":"The algorithm predicts the probability of each token being relevant (part of annotation). We then compare the predictions with the labels (correct values for each token) for the loss. To obtain the actual words, we need to extract the characters, which we can do using the offset mapping (for the relevant tokens), which we can the use to index the feature_text, and get the phrase, or compare the charcaters labels (0 or 1).\n","metadata":{}},{"cell_type":"code","source":"def train(model, dataloader, optimizer, criterion):\n    model.train()\n    train_loss=[]\n    \n    for batch in tqdm(dataloader):\n        optimizer.zero_grad()\n        inputs_ids= batch[0].to(device)\n        attention_mask=batch[1].to(device)\n        labels= batch[2].to(device)\n        offset_mapping= batch[3]\n        sequence_ids= batch[4]\n        \n        logits = model(input_ids, attention_mask)\n        loss= criterion(logits, labels)\n        loss = torch.masked_select(loss, labels > -1).mean()  #only selects the valid tokens\n        loss.backward()\n        optimizer.step()\n\n    return sum(train_loss)/len(train_loss)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid(model, dataloader, criterion):\n    model.eval()\n    valid_loss=[]\n    preds=[]\n    offsets=[]\n    seq_ids=[]\n    valid_labels=[]\n\n    for batch in tqdm(dataloader):\n            input_ids = batch[0].to(device)\n            attention_mask = batch[1].to(device)\n            token_type_ids = batch[2].to(device)\n            labels = batch[3].to(device)\n            offset_mapping = batch[4]\n            sequence_ids = batch[5]\n\n            logits = model(input_ids, attention_mask)\n            loss = criterion(logits, labels) #does sigmoid for us\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            valid_loss.append(loss.item() * input_ids.size(0))\n\n            preds.append(logits.detach().cpu().numpy())\n            offsets.append(offset_mapping.numpy())\n            seq_ids.append(sequence_ids.numpy())\n            valid_labels.append(labels.detach().cpu().numpy())\n\n    preds = np.concatenate(preds, axis=0)\n    offsets = np.concatenate(offsets, axis=0)\n    seq_ids = np.concatenate(seq_ids, axis=0)\n    valid_labels = np.concatenate(valid_labels, axis=0)\n    location_preds = get_location_predictions(preds, offsets, seq_ids, test=False)\n    score = calculate_char_cv(location_preds, offsets, seq_ids, valid_labels)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(3):\n    print(\"Epoch: {}/{}\".format(i + 1, epochs))\n    # first train model\n    train_loss = train(model, train_dataloader, optimizer, criterion)\n    train_loss_data.append(train_loss)\n    print(f\"Train loss: {train_loss}\")\n    # evaluate model\n    valid_loss, score = valid(model, test_dataloader, criterion)\n    valid_loss_data.append(valid_loss)\n    score_data_list.append(score)\n    print(f\"Valid loss: {valid_loss}\")\n    print(f\"Valid score: {score}\")\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For criterion, BCELoss (1 vs 0), for each token","metadata":{}}]}